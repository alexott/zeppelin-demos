{
  "paragraphs": [
    {
      "text": "%md\n\n# Work with Cassandra from Spark\n\nAccess to data in Cassandra from Spark is done via [DataStax Spark Cassandra Connector (SCC)](https://github.com/datastax/spark-cassandra-connector). \nAccess could be done via both APIs: RDD & Dataframe/Dataset.",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:54:20+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h1>Work with Cassandra from Spark</h1>\n<p>Access to data in Cassandra from Spark is done via <a href=\"https://github.com/datastax/spark-cassandra-connector\">DataStax Spark Cassandra Connector (SCC)</a>.<br />\nAccess could be done via both APIs: RDD &amp; Dataframe/Dataset.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1594623763695_-1066571472",
      "id": "20200303-131123_584760524",
      "dateCreated": "2020-07-13T07:02:43+0000",
      "dateStarted": "2020-07-13T07:12:52+0000",
      "dateFinished": "2020-07-13T07:12:52+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:26499"
    },
    {
      "text": "%md\n## Accessing Cassandra data via Dataframe/Dataset API\n\n[Documentation](https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/14_data_frames.md)\n\n### Reading data\n\nSCC integrates into standard Dataframe Spark API, so we can just use `spark.read`:\n\n* either explicitly specifying format & passing keyspace/table names via options:\n\n```scala\nval df = spark.read.format(\"org.apache.spark.sql.cassandra\").options(Map(\"table\" -> \"gen_events1\", \"keyspace\" -> \"test\")).load()\n```\n\n* or using auxiliary function `cassandraFormat`, that accepts talbe and keyspace names as arguments (please note that the first argument is the table name, and keyspace name - the second argument):\n\n```scala\nimport org.apache.spark.sql.cassandra._\nval df = spark.read.cassandraFormat(\"gen_events1\", \"test\").load()\n```\n\n\n### Writing data\n\nSimilar to reading, we can write data into Cassandra using the standard functions (table must already exist in Cassandra):\n\n```scala\ndf.write.format(\"org.apache.spark.sql.cassandra\").options(Map(\"table\" -> \"test_copy\", \"keyspace\" -> \"test\")).save()\n// or via auxiliary function:\nimport org.apache.spark.sql.cassandra._\ndf.write.cassandraFormat(\"test_copy\", \"test\").save()\n```\n\nBut we can also create the table in Cassandra based on the Dataframe's structure (although, it's better to use the `createCassandraTableEx` function, that supports more options):\n\n```scala\ndf.createCassandraTable( \"test\", \"new_table\", partitionKeyColumns = Some(Seq(\"id\")), clusteringKeyColumns = Some(Seq(\"new_col\")))\n```",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:54:35+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Accessing Cassandra data via Dataframe/Dataset API</h2>\n<p><a href=\"https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/14_data_frames.md\">Documentation</a></p>\n<h3>Reading data</h3>\n<p>SCC integrates into standard Dataframe Spark API, so we can just use <code>spark.read</code>:</p>\n<ul>\n<li>either explicitly specifying format &amp; passing keyspace/table names via options:</li>\n</ul>\n<pre><code class=\"language-scala\">val df = spark.read.format(&quot;org.apache.spark.sql.cassandra&quot;).options(Map(&quot;table&quot; -&gt; &quot;gen_events1&quot;, &quot;keyspace&quot; -&gt; &quot;test&quot;)).load()\n</code></pre>\n<ul>\n<li>or using auxiliary function <code>cassandraFormat</code>, that accepts talbe and keyspace names as arguments (please note that the first argument is the table name, and keyspace name - the second argument):</li>\n</ul>\n<pre><code class=\"language-scala\">import org.apache.spark.sql.cassandra._\nval df = spark.read.cassandraFormat(&quot;gen_events1&quot;, &quot;test&quot;).load()\n</code></pre>\n<h3>Writing data</h3>\n<p>Similar to reading, we can write data into Cassandra using the standard functions (table must already exist in Cassandra):</p>\n<pre><code class=\"language-scala\">df.write.format(&quot;org.apache.spark.sql.cassandra&quot;).options(Map(&quot;table&quot; -&gt; &quot;test_copy&quot;, &quot;keyspace&quot; -&gt; &quot;test&quot;)).save()\n// or via auxiliary function:\nimport org.apache.spark.sql.cassandra._\ndf.write.cassandraFormat(&quot;test_copy&quot;, &quot;test&quot;).save()\n</code></pre>\n<p>But we can also create the table in Cassandra based on the Dataframe&rsquo;s structure (although, it&rsquo;s better to use the <code>createCassandraTableEx</code> function, that supports more options):</p>\n<pre><code class=\"language-scala\">df.createCassandraTable( &quot;test&quot;, &quot;new_table&quot;, partitionKeyColumns = Some(Seq(&quot;id&quot;)), clusteringKeyColumns = Some(Seq(&quot;new_col&quot;)))\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1594623763699_2034004323",
      "id": "20200304-104356_1510767338",
      "dateCreated": "2020-07-13T07:02:43+0000",
      "dateStarted": "2020-07-13T07:20:07+0000",
      "dateFinished": "2020-07-13T07:20:07+0000",
      "status": "FINISHED",
      "$$hashKey": "object:26500"
    },
    {
      "text": "import org.apache.spark.sql.cassandra._\nval df = spark.read.cassandraFormat(\"gen_events1\", \"test\").load()\n   .select(\"event_type\", \"day\", \"time_of_occurrence\", \"point\")\n   .where(\"event_type = 'fog'\")\nz.show(df)",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:02:43+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "event_type": "string",
                      "day": "string",
                      "time_of_occurrence": "string",
                      "point": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          },
          "1": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {}
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "event_type\tday\ttime_of_occurrence\tpoint\nfog\t2018-10-22\t2018-07-18 10:00:00.0\tnull\nfog\t2018-10-22\t2018-07-18 09:00:00.0\tnull\n"
          },
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.cassandra._\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [event_type: string, day: date ... 2 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1594623763699_18484817",
      "id": "20200304-105943_1107358832",
      "dateCreated": "2020-07-13T07:02:43+0000",
      "status": "READY",
      "$$hashKey": "object:26501"
    },
    {
      "text": "%md\n\n### Filtering of results, and predicates pushdown in Spark SQL/Dataframe API\n\nWhen we're doing selection of data from Cassandra, SCC is trying to perform so-called predicate pushdown to minimize the amount of data read from a table - if it's possible, then data filtering happens when performing a query. Because this filtering happens inside Cassandra, we're getting better performance, although applicability of these filters may depend on the table structure. Usually, if we can perform query using CQL, then predicate pushdown will happen). Full list of conditions for performing predicate pushdown could be found in [documentation](https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/14_data_frames.md#full-list-of-predicate-pushdown-restrictions).\n\nWe can check if predicate pushdown happens by executing `explain` on dataframe - conditions for which predicate pushdown happens will be marked with `*` character:",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:28:19+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Filtering of results, and predicates pushdown in Spark SQL/Dataframe API</h3>\n<p>When we&rsquo;re doing selection of data from Cassandra, SCC is trying to perform so-called predicate pushdown to minimize the amount of data read from a table - if it&rsquo;s possible, then data filtering happens when performing a query. Because this filtering happens inside Cassandra, we&rsquo;re getting better performance, although applicability of these filters may depend on the table structure. Usually, if we can perform query using CQL, then predicate pushdown will happen). Full list of conditions for performing predicate pushdown could be found in <a href=\"https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/14_data_frames.md#full-list-of-predicate-pushdown-restrictions\">documentation</a>.</p>\n<p>We can check if predicate pushdown happens by executing <code>explain</code> on dataframe - conditions for which predicate pushdown happens will be marked with <code>*</code> character:</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1594623763699_97767715",
      "id": "20200304-110134_941805320",
      "dateCreated": "2020-07-13T07:02:43+0000",
      "dateStarted": "2020-07-13T07:27:39+0000",
      "dateFinished": "2020-07-13T07:27:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:26502"
    },
    {
      "text": "import org.apache.spark.sql.cassandra._\n// с predicate pushdown (имеется значок '*' на PushedFilters)\nval df = spark.read.cassandraFormat(\"gen_events1\", \"test\").load()\n   .select(\"event_type\", \"day\", \"time_of_occurrence\", \"point\")\n   .where(\"time_of_occurrence >= cast('2018-02-17T00:00:00Z' as timestamp) AND time_of_occurrence <= cast('2020-02-17T00:00:00Z' as timestamp)\")\n   .explain()",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:02:43+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "== Physical Plan ==\n*Scan org.apache.spark.sql.cassandra.CassandraSourceRelation [event_type#1414,day#1413,time_of_occurrence#1415,point#1418] PushedFilters: [*GreaterThanOrEqual(time_of_occurrence,2018-02-17 00:00:00.0), *LessThanOrEqual(time_of_occurren..., ReadSchema: struct<event_type:string,day:date,time_of_occurrence:timestamp,point:string>\nimport org.apache.spark.sql.cassandra._\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32mUnit\u001b[0m = ()\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1594623763700_939709766",
      "id": "20200304-110717_1911940439",
      "dateCreated": "2020-07-13T07:02:43+0000",
      "status": "READY",
      "$$hashKey": "object:26503"
    },
    {
      "text": "import org.apache.spark.sql.cassandra._\n// без predicate pushdown (нет значка '*' для PushedFilters)\nval df = spark.read.cassandraFormat(\"gen_events1\", \"test\").load()\n   .select(\"event_type\", \"day\", \"time_of_occurrence\", \"point\")\n   .where(\"event_type = 'fog'\")\n   .explain()",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:02:43+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "== Physical Plan ==\n*Filter (event_type#1438 = fog)\n+- *Scan org.apache.spark.sql.cassandra.CassandraSourceRelation [event_type#1438,day#1437,time_of_occurrence#1439,point#1442] PushedFilters: [EqualTo(event_type,fog)], ReadSchema: struct<event_type:string,day:date,time_of_occurrence:timestamp,point:string>\nimport org.apache.spark.sql.cassandra._\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32mUnit\u001b[0m = ()\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1594623763700_673626106",
      "id": "20200304-113822_216765961",
      "dateCreated": "2020-07-13T07:02:43+0000",
      "status": "READY",
      "$$hashKey": "object:26504"
    },
    {
      "text": "%md\n\n## Working with Cassandra/DSE from Spark SQL\n\nBesides using Spark API, we can also work with Cassandra via Spark SQL. If we don't use DSE Analytics, where Cassandra tables are registered automatically, then we need to register these tables explicitly as [described in documentation](https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/14_data_frames.md#creating-datasets-using-spark-sql) (this happens automatically in Spark 3.0 & SCC 3.0). After the table is registered, we can perform queries against it:",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:42:09+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Working with Cassandra/DSE from Spark SQL</h2>\n<p>Besides using Spark API, we can also work with Cassandra via Spark SQL. If we don&rsquo;t use DSE Analytics, where Cassandra tables are registered automatically, then we need to register these tables explicitly as <a href=\"https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/14_data_frames.md#creating-datasets-using-spark-sql\">described in documentation</a> (this happens automatically in Spark 3.0 &amp; SCC 3.0). After the table is registered, we can perform queries against it:</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1594623763700_1173453665",
      "id": "20200303-131147_39822417",
      "dateCreated": "2020-07-13T07:02:43+0000",
      "dateStarted": "2020-07-13T07:30:48+0000",
      "dateFinished": "2020-07-13T07:30:48+0000",
      "status": "FINISHED",
      "$$hashKey": "object:26505"
    },
    {
      "text": "%spark.sql\n\nCREATE or replace TEMPORARY VIEW generic_events USING org.apache.spark.sql.cassandra \n  OPTIONS ( table \"gen_events1\", keyspace \"test\", pushdown \"true\")",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:02:43+0000",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sql",
        "fontSize": 9,
        "editorHide": false,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1594623763700_1223272031",
      "id": "20200304-103305_1739670646",
      "dateCreated": "2020-07-13T07:02:43+0000",
      "status": "READY",
      "$$hashKey": "object:26506"
    },
    {
      "text": "%spark.sql\nselect event_type, day, time_of_occurrence, point from generic_events where event_type = 'fog' limit 25",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:02:43+0000",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sql",
        "fontSize": 9,
        "editorHide": false,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "event_type": "string",
                      "day": "string",
                      "time_of_occurrence": "string",
                      "point": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "event_type\tday\ttime_of_occurrence\tpoint\nfog\t2018-10-22\t2018-07-18 10:00:00.0\tnull\nfog\t2018-10-22\t2018-07-18 09:00:00.0\tnull\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1594623763700_-236641507",
      "id": "20200304-103352_1844360462",
      "dateCreated": "2020-07-13T07:02:43+0000",
      "status": "READY",
      "$$hashKey": "object:26507"
    },
    {
      "text": "%md\n\n## Performing the join with data in Cassandra\n\nStarting with SCC 2.5.0 we can perform effective join of any dataframe with data in Cassandra - previously this functionality was available only in DSE Analytics, or required switching to RDD API.\n\nJoin with data in Cassandra very often used for data enrichment, for example, if we have a data stream, and we need to enrich it with some information.  For example, this could be a stream with information about buying and selling of stocks, and we have only stock ticker, but not the extended information that is stored in Cassandra.  When join is based on the primary or partition key, then SCC transforms such  join into queries against individual rows or partitions, and don't read the whole table into Spark - as result, such join is very fast.\n\nWe can check, if such join happens by executing `explain`, and checking that we have lines `DSE Direct Join` or `Cassandra Direct Join` in the output, as it shown below.",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:37:21+0000",
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Performing the join with data in Cassandra</h2>\n<p>Starting with SCC 2.5.0 we can perform effective join of any dataframe with data in Cassandra - previously this functionality was available only in DSE Analytics, or required switching to RDD API.</p>\n<p>Join with data in Cassandra very often used for data enrichment, for example, if we have a data stream, and we need to enrich it with some information.  For example, this could be a stream with information about buying and selling of stocks, and we have only stock ticker, but not the extended information that is stored in Cassandra.  When join is based on the primary or partition key, then SCC transforms such  join into queries against individual rows or partitions, and don&rsquo;t read the whole table into Spark - as result, such join is very fast.</p>\n<p>We can check, if such join happens by executing <code>explain</code>, and checking that we have lines <code>DSE Direct Join</code> or <code>Cassandra Direct Join</code> in the output, as it shown below.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1594623763700_-1617275336",
      "id": "paragraph_1590660951296_464749634",
      "dateCreated": "2020-07-13T07:02:43+0000",
      "dateStarted": "2020-07-13T07:37:18+0000",
      "dateFinished": "2020-07-13T07:37:18+0000",
      "status": "FINISHED",
      "$$hashKey": "object:26508"
    },
    {
      "text": "%spark\n\nimport spark.implicits._\n\nval toJoin = spark.range(1, 10).map(x => x.intValue).withColumnRenamed(\"value\", \"id\")\n\nval dataset = spark.read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map(\"table\" -> \"jtest\", \"keyspace\" -> \"test\"))\n  .load\nz.show(dataset.filter($\"id\" <= 10))",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:02:43+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {
                    "columns": [
                      {
                        "name": "id0",
                        "visible": true,
                        "width": "*",
                        "sort": {
                          "priority": 0,
                          "direction": "desc"
                        },
                        "filters": [
                          {}
                        ],
                        "pinned": ""
                      },
                      {
                        "name": "t1",
                        "visible": true,
                        "width": "*",
                        "sort": {},
                        "filters": [
                          {}
                        ],
                        "pinned": ""
                      }
                    ],
                    "scrollFocus": {},
                    "selection": [],
                    "grouping": {
                      "grouping": [],
                      "aggregations": [],
                      "rowExpandedStates": {}
                    },
                    "treeView": {},
                    "pagination": {
                      "paginationCurrentPage": 1,
                      "paginationPageSize": 250
                    }
                  },
                  "tableColumnTypeState": {
                    "names": {
                      "id": "string",
                      "t": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "id\tt\n1\tt1\n8\tt8\n0\tt0\n2\tt2\n7\tt7\n6\tt6\n3\tt3\n5\tt5\n10\tt10\n4\tt4\n9\tt9\n"
          },
          {
            "type": "TEXT",
            "data": "import spark.implicits._\n\u001b[1m\u001b[34mtoJoin\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [id: int]\n\u001b[1m\u001b[34mdataset\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [id: int, t: string]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1594623763700_-2083125681",
      "id": "paragraph_1590661006773_202269590",
      "dateCreated": "2020-07-13T07:02:43+0000",
      "status": "READY",
      "$$hashKey": "object:26509"
    },
    {
      "text": "%spark\nval joined = toJoin.join(dataset, dataset(\"id\") === toJoin(\"id\"))\njoined.explain\nz.show(joined)",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:02:43+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {
          "1": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "id": "string",
                      "t": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "== Physical Plan ==\nDSE Direct Join [id = id#1518] test.jtest - Reading (id, t) Pushed {} \n+- *Project [value#1516 AS id#1518]\n   +- *SerializeFromObject [input[0, int, false] AS value#1516]\n      +- *MapElements <function1>, obj#1515: int\n         +- *DeserializeToObject staticinvoke(class java.lang.Long, ObjectType(class java.lang.Long), valueOf, id#1510L, true), obj#1514: java.lang.Long\n            +- *Range (1, 10, step=1, splits=3)\n\n"
          },
          {
            "type": "TABLE",
            "data": "id\tid\tt\n1\t1\tt1\n2\t2\tt2\n3\t3\tt3\n4\t4\tt4\n5\t5\tt5\n6\t6\tt6\n7\t7\tt7\n8\t8\tt8\n9\t9\tt9\n"
          },
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mjoined\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [id: int, id: int ... 1 more field]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1594623763700_-1253501528",
      "id": "paragraph_1590661040495_-674391827",
      "dateCreated": "2020-07-13T07:02:43+0000",
      "status": "READY",
      "$$hashKey": "object:26510"
    },
    {
      "text": "%md\n\n## Using the RDD API\n\nNow, it makes sense to use RDD API only in a limited set of the scenarios, because Spark usually can better optimize code that uses Dataframes or Spark SQL (before SCC 2.5.0 some of the things below were available only in RDD API):\n\n* (pre-2.5.0) when we were need to access to metadata, such as TTL and WriteTime ([documentation](https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/3_selection.md#example-using-select-to-retreive-ttl-and-timestamp))\n* (pre-2.5.0) when we were need to perform effective join with data in Cassandra\n* when we need to delete data from a table ([documentation](https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/5_saving.md#deleting-rows-and-columns)).\n* when we need to perform partial update of column of collection type (add or delete individual elements)",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:44:45+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Using the RDD API</h2>\n<p>Now, it makes sense to use RDD API only in a limited set of the scenarios, because Spark usually can better optimize code that uses Dataframes or Spark SQL (before SCC 2.5.0 some of the things below were available only in RDD API):</p>\n<ul>\n<li>(pre-2.5.0) when we were need to access to metadata, such as TTL and WriteTime (<a href=\"https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/3_selection.md#example-using-select-to-retreive-ttl-and-timestamp\">documentation</a>)</li>\n<li>(pre-2.5.0) when we were need to perform effective join with data in Cassandra</li>\n<li>when we need to delete data from a table (<a href=\"https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/5_saving.md#deleting-rows-and-columns\">documentation</a>).</li>\n<li>when we need to perform partial update of column of collection type (add or delete individual elements)</li>\n</ul>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1594623763700_-1461321932",
      "id": "20200304-103507_803705969",
      "dateCreated": "2020-07-13T07:02:43+0000",
      "dateStarted": "2020-07-13T07:44:44+0000",
      "dateFinished": "2020-07-13T07:44:44+0000",
      "status": "FINISHED",
      "$$hashKey": "object:26511"
    },
    {
      "title": "Reading data into RDD[CassandraRow]",
      "text": "%spark\n\nimport com.datastax.spark.connector._\n\nval data = sc.cassandraTable(\"test\", \"jtest\")",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:37:47+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import com.datastax.spark.connector._\n\u001b[1m\u001b[34mdata\u001b[0m: \u001b[1m\u001b[32mcom.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow]\u001b[0m = CassandraTableScanRDD[1385] at RDD at CassandraRDD.scala:19\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1594623763701_-568738858",
      "id": "paragraph_1590682635924_883647425",
      "dateCreated": "2020-07-13T07:02:43+0000",
      "status": "READY",
      "$$hashKey": "object:26512"
    },
    {
      "title": "Writing data into Cassandra",
      "text": "%spark\ndata.saveToCassandra(\"test\", \"jtest\")",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:37:59+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1594623763701_772311071",
      "id": "paragraph_1590682954084_-918753717",
      "dateCreated": "2020-07-13T07:02:43+0000",
      "status": "READY",
      "$$hashKey": "object:26513"
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:02:43+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1594623763701_-1502594609",
      "id": "paragraph_1590741544473_-521640328",
      "dateCreated": "2020-07-13T07:02:43+0000",
      "status": "READY",
      "$$hashKey": "object:26514"
    }
  ],
  "name": "Cassandra Day Russia: Spark + Cassandra (English)",
  "id": "2FF8H8NCE",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-preview1",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/Cassandra Day Russia: Spark + Cassandra (English)"
}