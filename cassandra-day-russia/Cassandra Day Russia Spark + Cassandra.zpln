{
  "paragraphs": [
    {
      "text": "%md\n\n# Работа с Cassandra из Spark\n\nДоступ к данным в Cassandra из Spark осуществляется через [DataStax Spark Cassandra Connector (SCC)](https://github.com/datastax/spark-cassandra-connector). \nДоступ может осуществляться через оба API: RDD & Dataframe/Dataset.",
      "user": "anonymous",
      "dateUpdated": "2020-05-29T08:37:56+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h1>Работа с Cassandra из Spark</h1>\n<p>Доступ к данным в Cassandra из Spark осуществляется через <a href=\"https://github.com/datastax/spark-cassandra-connector\">DataStax Spark Cassandra Connector (SCC)</a>.<br />\nДоступ может осуществляться через оба API: RDD &amp; Dataframe/Dataset.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1590660402913_-537715507",
      "id": "20200303-131123_584760524",
      "dateCreated": "2020-05-28T10:06:42+0000",
      "dateStarted": "2020-05-29T08:37:56+0000",
      "dateFinished": "2020-05-29T08:37:56+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:8227"
    },
    {
      "text": "%md\n## Доступ к даннным в Cassandra через Dataframe/Dataset API\n\n### Чтение данных\n\nSCC интегрируется со стандартным Dataframe Spark API, так что мы можем просто использовать `spark.read`:\n\n* либо явно указывая формат и передавая название таблицы и keyspace через опции:\n\n```scala\nval df = spark.read.format(\"org.apache.spark.sql.cassandra\").options(Map(\"table\" -> \"gen_events1\", \"keyspace\" -> \"test\")).load()\n```\n\n* либо используя вспомогатаельную функцию `cassandraFormat`, которая принимает название таблицы и keyspace в качестве аргументов (заметьте, что первый аргумент - это название таблицы, а название keyspace - это второй):\n\n```scala\nimport org.apache.spark.sql.cassandra._\nval df = spark.read.cassandraFormat(\"gen_events1\", \"test\").load()\n```\n\n\n### Запись данных\n\nАналогично чтению, мы можем записать данные в Cassandra используя стандартные функции (таблица уже должна существовать Cassandra):\n\n```scala\ndf.write.format(\"org.apache.spark.sql.cassandra\").options(Map(\"table\" -> \"test_copy\", \"keyspace\" -> \"test\")).save()\n// или используя вспомогатаельную функцию:\nimport org.apache.spark.sql.cassandra._\ndf.write.cassandraFormat(\"test_copy\", \"test\").save()\n```\n\nНо мы также можем создать таблицу на основе структуры Dataframe (хотя лучше использовать функцию `createCassandraTableEx`, поскольку она позволяет указать больше опций):\n\n```scala\ndf.createCassandraTable( \"test\", \"new_table\", partitionKeyColumns = Some(Seq(\"id\")), clusteringKeyColumns = Some(Seq(“new_col\")))\n```\n",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:54:53+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Доступ к даннным в Cassandra через Dataframe/Dataset API</h2>\n<h3>Чтение данных</h3>\n<p>SCC интегрируется со стандартным Dataframe Spark API, так что мы можем просто использовать <code>spark.read</code>:</p>\n<ul>\n<li>либо явно указывая формат и передавая название таблицы и keyspace через опции:</li>\n</ul>\n<pre><code class=\"language-scala\">val df = spark.read.format(&quot;org.apache.spark.sql.cassandra&quot;).options(Map(&quot;table&quot; -&gt; &quot;gen_events1&quot;, &quot;keyspace&quot; -&gt; &quot;test&quot;)).load()\n</code></pre>\n<ul>\n<li>либо используя вспомогатаельную функцию <code>cassandraFormat</code>, которая принимает название таблицы и keyspace в качестве аргументов (заметьте, что первый аргумент - это название таблицы, а название keyspace - это второй):</li>\n</ul>\n<pre><code class=\"language-scala\">import org.apache.spark.sql.cassandra._\nval df = spark.read.cassandraFormat(&quot;gen_events1&quot;, &quot;test&quot;).load()\n</code></pre>\n<h3>Запись данных</h3>\n<p>Аналогично чтению, мы можем записать данные в Cassandra используя стандартные функции (таблица уже должна существовать Cassandra):</p>\n<pre><code class=\"language-scala\">df.write.format(&quot;org.apache.spark.sql.cassandra&quot;).options(Map(&quot;table&quot; -&gt; &quot;test_copy&quot;, &quot;keyspace&quot; -&gt; &quot;test&quot;)).save()\n// или используя вспомогатаельную функцию:\nimport org.apache.spark.sql.cassandra._\ndf.write.cassandraFormat(&quot;test_copy&quot;, &quot;test&quot;).save()\n</code></pre>\n<p>Но мы также можем создать таблицу на основе структуры Dataframe (хотя лучше использовать функцию <code>createCassandraTableEx</code>, поскольку она позволяет указать больше опций):</p>\n<pre><code class=\"language-scala\">df.createCassandraTable( &quot;test&quot;, &quot;new_table&quot;, partitionKeyColumns = Some(Seq(&quot;id&quot;)), clusteringKeyColumns = Some(Seq(“new_col&quot;)))\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1590660402916_-1026290278",
      "id": "20200304-104356_1510767338",
      "dateCreated": "2020-05-28T10:06:42+0000",
      "dateStarted": "2020-05-29T08:37:56+0000",
      "dateFinished": "2020-05-29T08:37:56+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8228"
    },
    {
      "text": "import org.apache.spark.sql.cassandra._\nval df = spark.read.cassandraFormat(\"gen_events1\", \"test\").load()\n   .select(\"event_type\", \"day\", \"time_of_occurrence\", \"point\")\n   .where(\"event_type = 'fog'\")\nz.show(df)",
      "user": "anonymous",
      "dateUpdated": "2020-05-29T09:44:23+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "event_type": "string",
                      "day": "string",
                      "time_of_occurrence": "string",
                      "point": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          },
          "1": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {}
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "event_type\tday\ttime_of_occurrence\tpoint\nfog\t2018-10-22\t2018-07-18 10:00:00.0\tnull\nfog\t2018-10-22\t2018-07-18 09:00:00.0\tnull\n"
          },
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.cassandra._\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [event_type: string, day: date ... 2 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1590660402916_-1858541457",
      "id": "20200304-105943_1107358832",
      "dateCreated": "2020-05-28T10:06:42+0000",
      "dateStarted": "2020-05-29T09:44:23+0000",
      "dateFinished": "2020-05-29T09:44:25+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8229"
    },
    {
      "text": "%md\n\n### Фильтрация результатов и predicates pushdown в Spark SQL/Dataframe API\n\nКогда мы выполняем запрос данных хранимых в таблице Cassandra, SCC пытается выполнить так называемый predicate pushdown чтобы минимизовать объем данных считываемых из таблицы - если это получается, то фильтрация данных происходит при чтении. Поскольку эти фильтры применяются внутри Cassandra, мы получаем лучшую производительность, хотя применимость этих фильтров может зависеть от структуры таблицы. На практике можно сказать, что если мы можем выполнить запрос как CQL, то predicate pushdown случится). Полный список условий когда происходит predicate pushdown [можно найти в документации](https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/14_data_frames.md#full-list-of-predicate-pushdown-restrictions).\n\nМы можем посмотреть, выполняется ли predicate pushdown если мы выполним `explain` для dataframe - условия для которых происходит predicate pushdown будут отмечены значком `*` :",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:55:06+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Фильтрация результатов и predicates pushdown в Spark SQL/Dataframe API</h3>\n<p>Когда мы выполняем запрос данных хранимых в таблице Cassandra, SCC пытается выполнить так называемый predicate pushdown чтобы минимизовать объем данных считываемых из таблицы - если это получается, то фильтрация данных происходит при чтении. Поскольку эти фильтры применяются внутри Cassandra, мы получаем лучшую производительность, хотя применимость этих фильтров может зависеть от структуры таблицы. На практике можно сказать, что если мы можем выполнить запрос как CQL, то predicate pushdown случится). Полный список условий когда происходит predicate pushdown <a href=\"https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/14_data_frames.md#full-list-of-predicate-pushdown-restrictions\">можно найти в документации</a>.</p>\n<p>Мы можем посмотреть, выполняется ли predicate pushdown если мы выполним <code>explain</code> для dataframe - условия для которых происходит predicate pushdown будут отмечены значком <code>*</code> :</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1590660402918_525714847",
      "id": "20200304-110134_941805320",
      "dateCreated": "2020-05-28T10:06:42+0000",
      "dateStarted": "2020-05-29T08:38:08+0000",
      "dateFinished": "2020-05-29T08:38:08+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8230"
    },
    {
      "text": "import org.apache.spark.sql.cassandra._\n// с predicate pushdown (имеется значок '*' на PushedFilters)\nval df = spark.read.cassandraFormat(\"gen_events1\", \"test\").load()\n   .select(\"event_type\", \"day\", \"time_of_occurrence\", \"point\")\n   .where(\"time_of_occurrence >= cast('2018-02-17T00:00:00Z' as timestamp) AND time_of_occurrence <= cast('2020-02-17T00:00:00Z' as timestamp)\")\n   .explain()",
      "user": "anonymous",
      "dateUpdated": "2020-05-29T09:45:57+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "== Physical Plan ==\n*Scan org.apache.spark.sql.cassandra.CassandraSourceRelation [event_type#1414,day#1413,time_of_occurrence#1415,point#1418] PushedFilters: [*GreaterThanOrEqual(time_of_occurrence,2018-02-17 00:00:00.0), *LessThanOrEqual(time_of_occurren..., ReadSchema: struct<event_type:string,day:date,time_of_occurrence:timestamp,point:string>\nimport org.apache.spark.sql.cassandra._\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32mUnit\u001b[0m = ()\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1590660402919_-1844709080",
      "id": "20200304-110717_1911940439",
      "dateCreated": "2020-05-28T10:06:42+0000",
      "dateStarted": "2020-05-29T09:45:57+0000",
      "dateFinished": "2020-05-29T09:45:58+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8231"
    },
    {
      "text": "import org.apache.spark.sql.cassandra._\n// без predicate pushdown (нет значка '*' для PushedFilters)\nval df = spark.read.cassandraFormat(\"gen_events1\", \"test\").load()\n   .select(\"event_type\", \"day\", \"time_of_occurrence\", \"point\")\n   .where(\"event_type = 'fog'\")\n   .explain()",
      "user": "anonymous",
      "dateUpdated": "2020-05-29T09:46:38+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "== Physical Plan ==\n*Filter (event_type#1438 = fog)\n+- *Scan org.apache.spark.sql.cassandra.CassandraSourceRelation [event_type#1438,day#1437,time_of_occurrence#1439,point#1442] PushedFilters: [EqualTo(event_type,fog)], ReadSchema: struct<event_type:string,day:date,time_of_occurrence:timestamp,point:string>\nimport org.apache.spark.sql.cassandra._\n\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32mUnit\u001b[0m = ()\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1590660402919_-85046262",
      "id": "20200304-113822_216765961",
      "dateCreated": "2020-05-28T10:06:42+0000",
      "dateStarted": "2020-05-29T09:46:38+0000",
      "dateFinished": "2020-05-29T09:46:38+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8232"
    },
    {
      "text": "%md\n\n## Работа с Cassandra/DSE из Spark SQL\n\nПомимо работы напрямую из кода Spark, мы также можем работать с данными в Cassandra через Spark SQL. В том случае если не используется DSE Analytics, где таблицы регистрируются автоматически, нам нужно явно зарегистрировать таблицы Cassandra как [описано в документации](https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/14_data_frames.md#creating-datasets-using-spark-sql). После того, как таблица зарегистрирована, мы можем выполнять запросы к ней:",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:55:18+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Работа с Cassandra/DSE из Spark SQL</h2>\n<p>Помимо работы напрямую из кода Spark, мы также можем работать с данными в Cassandra через Spark SQL. В том случае если не используется DSE Analytics, где таблицы регистрируются автоматически, нам нужно явно зарегистрировать таблицы Cassandra как <a href=\"https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/14_data_frames.md#creating-datasets-using-spark-sql\">описано в документации</a>. После того, как таблица зарегистрирована, мы можем выполнять запросы к ней:</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1590660402919_952576307",
      "id": "20200303-131147_39822417",
      "dateCreated": "2020-05-28T10:06:42+0000",
      "dateStarted": "2020-07-13T07:55:15+0000",
      "dateFinished": "2020-07-13T07:55:15+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8233"
    },
    {
      "text": "%spark.sql\n\nCREATE or replace TEMPORARY VIEW generic_events USING org.apache.spark.sql.cassandra \n  OPTIONS ( table \"gen_events1\", keyspace \"test\", pushdown \"true\")",
      "user": "anonymous",
      "dateUpdated": "2020-05-29T09:47:39+0000",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sql",
        "fontSize": 9,
        "editorHide": false,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1590660402920_1964628205",
      "id": "20200304-103305_1739670646",
      "dateCreated": "2020-05-28T10:06:42+0000",
      "dateStarted": "2020-05-29T09:47:39+0000",
      "dateFinished": "2020-05-29T09:47:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8234"
    },
    {
      "text": "%spark.sql\nselect event_type, day, time_of_occurrence, point from generic_events where event_type = 'fog' limit 25",
      "user": "anonymous",
      "dateUpdated": "2020-05-29T09:47:41+0000",
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sql",
        "fontSize": 9,
        "editorHide": false,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "event_type": "string",
                      "day": "string",
                      "time_of_occurrence": "string",
                      "point": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "event_type\tday\ttime_of_occurrence\tpoint\nfog\t2018-10-22\t2018-07-18 10:00:00.0\tnull\nfog\t2018-10-22\t2018-07-18 09:00:00.0\tnull\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1590660402920_-603171132",
      "id": "20200304-103352_1844360462",
      "dateCreated": "2020-05-28T10:06:42+0000",
      "dateStarted": "2020-05-29T09:47:41+0000",
      "dateFinished": "2020-05-29T09:47:42+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8235"
    },
    {
      "text": "%md\n\n## Выполнение join с данными в Cassandra\n\nНачиная с SCC 2.5.0 мы можем выполнять эффективный join dataframe с данными в Cassandra - эта функциональность ранее была доступна только в DSE Analytics, или надо было использовать RDD API для работы с Cassandra.\n\nJoin данных с Cassandra часто используется для обогащения данных, например, когда у нас есть поток данных, к которым надо добавить какую-то информацию.  Например, это может быть поток данных о продаже и покупке акци, и мы имеем только сокращенное имя компании, но не остальную информацию, которая хранится в Cassandra.  В том случае когда join происходит по primary или partition key, то SCC преобразует такой join в запросы к отдельным строкам или разделам, а не читает все данные в Spark, и соотвественно такой join происходит очень быстро.\n\nМы всегда можем проверить, происходит ли join путем выполнения `explain`, и проверке наличия строки `DSE Direct Join` или `Cassandra Direct Join` в выводе.",
      "user": "anonymous",
      "dateUpdated": "2020-05-29T08:38:11+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/text",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Выполнение join с данными в Cassandra</h2>\n<p>Начиная с SCC 2.5.0 мы можем выполнять эффективный join dataframe с данными в Cassandra - эта функциональность ранее была доступна только в DSE Analytics, или надо было использовать RDD API для работы с Cassandra.</p>\n<p>Join данных с Cassandra часто используется для обогащения данных, например, когда у нас есть поток данных, к которым надо добавить какую-то информацию.  Например, это может быть поток данных о продаже и покупке акци, и мы имеем только сокращенное имя компании, но не остальную информацию, которая хранится в Cassandra.  В том случае когда join происходит по primary или partition key, то SCC преобразует такой join в запросы к отдельным строкам или разделам, а не читает все данные в Spark, и соотвественно такой join происходит очень быстро.</p>\n<p>Мы всегда можем проверить, происходит ли join путем выполнения <code>explain</code>, и проверке наличия строки <code>DSE Direct Join</code> или <code>Cassandra Direct Join</code> в выводе.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1590660951296_464749634",
      "id": "paragraph_1590660951296_464749634",
      "dateCreated": "2020-05-28T10:15:51+0000",
      "dateStarted": "2020-05-29T08:38:11+0000",
      "dateFinished": "2020-05-29T08:38:11+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8236"
    },
    {
      "text": "%spark\n\nimport spark.implicits._\n\nval toJoin = spark.range(1, 10).map(x => x.intValue).withColumnRenamed(\"value\", \"id\")\n\nval dataset = spark.read\n  .format(\"org.apache.spark.sql.cassandra\")\n  .options(Map(\"table\" -> \"jtest\", \"keyspace\" -> \"test\"))\n  .load\nz.show(dataset.filter($\"id\" <= 10))",
      "user": "anonymous",
      "dateUpdated": "2020-05-29T09:48:19+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {
                    "columns": [
                      {
                        "name": "id0",
                        "visible": true,
                        "width": "*",
                        "sort": {
                          "priority": 0,
                          "direction": "desc"
                        },
                        "filters": [
                          {}
                        ],
                        "pinned": ""
                      },
                      {
                        "name": "t1",
                        "visible": true,
                        "width": "*",
                        "sort": {},
                        "filters": [
                          {}
                        ],
                        "pinned": ""
                      }
                    ],
                    "scrollFocus": {},
                    "selection": [],
                    "grouping": {
                      "grouping": [],
                      "aggregations": [],
                      "rowExpandedStates": {}
                    },
                    "treeView": {},
                    "pagination": {
                      "paginationCurrentPage": 1,
                      "paginationPageSize": 250
                    }
                  },
                  "tableColumnTypeState": {
                    "names": {
                      "id": "string",
                      "t": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "id\tt\n1\tt1\n8\tt8\n0\tt0\n2\tt2\n7\tt7\n6\tt6\n3\tt3\n5\tt5\n10\tt10\n4\tt4\n9\tt9\n"
          },
          {
            "type": "TEXT",
            "data": "import spark.implicits._\n\u001b[1m\u001b[34mtoJoin\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [id: int]\n\u001b[1m\u001b[34mdataset\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [id: int, t: string]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1590661006773_202269590",
      "id": "paragraph_1590661006773_202269590",
      "dateCreated": "2020-05-28T10:16:46+0000",
      "dateStarted": "2020-05-29T09:48:19+0000",
      "dateFinished": "2020-05-29T09:48:21+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8237"
    },
    {
      "text": "%spark\nval joined = toJoin.join(dataset, dataset(\"id\") === toJoin(\"id\"))\njoined.explain\nz.show(joined)",
      "user": "anonymous",
      "dateUpdated": "2020-05-29T09:48:38+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {
          "1": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "id": "string",
                      "t": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "== Physical Plan ==\nDSE Direct Join [id = id#1518] test.jtest - Reading (id, t) Pushed {} \n+- *Project [value#1516 AS id#1518]\n   +- *SerializeFromObject [input[0, int, false] AS value#1516]\n      +- *MapElements <function1>, obj#1515: int\n         +- *DeserializeToObject staticinvoke(class java.lang.Long, ObjectType(class java.lang.Long), valueOf, id#1510L, true), obj#1514: java.lang.Long\n            +- *Range (1, 10, step=1, splits=3)\n\n"
          },
          {
            "type": "TABLE",
            "data": "id\tid\tt\n1\t1\tt1\n2\t2\tt2\n3\t3\tt3\n4\t4\tt4\n5\t5\tt5\n6\t6\tt6\n7\t7\tt7\n8\t8\tt8\n9\t9\tt9\n"
          },
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mjoined\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [id: int, id: int ... 1 more field]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1590661040495_-674391827",
      "id": "paragraph_1590661040495_-674391827",
      "dateCreated": "2020-05-28T10:17:20+0000",
      "dateStarted": "2020-05-29T09:48:38+0000",
      "dateFinished": "2020-05-29T09:48:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8238"
    },
    {
      "text": "%md\n\n## Использование RDD API\n\nRDD API обычно имеет смысл использовать только в ограниченном количестве сценариев, поскольку обычно Spark может сильнее оптимизовать код использующий dataframe или Spark SQL (до выхода SCC 2.5.0 некоторые из этих функций были доступны только в RDD API):\n\n* (pre-2.5.0) когда нам нужно получать доступ к метаданным, таким как TTL и WriteTime ([documentation](https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/3_selection.md#example-using-select-to-retreive-ttl-and-timestamp))\n* (pre-2.5.0) когда нам нужно выполнить эффективный join с данными в Cassandra\n* когда мы хотим удалить данные из таблицы ([документация](https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/5_saving.md#deleting-rows-and-columns))\n* когда мы хотим выполнить частичное обновление колонок с типами данных `list`/`map`/`set` (добавить или удалить отдельные элементы)",
      "user": "anonymous",
      "dateUpdated": "2020-07-13T07:55:39+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Использование RDD API</h2>\n<p>RDD API обычно имеет смысл использовать только в ограниченном количестве сценариев, поскольку обычно Spark может сильнее оптимизовать код использующий dataframe или Spark SQL (до выхода SCC 2.5.0 некоторые из этих функций были доступны только в RDD API):</p>\n<ul>\n<li>(pre-2.5.0) когда нам нужно получать доступ к метаданным, таким как TTL и WriteTime (<a href=\"https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/3_selection.md#example-using-select-to-retreive-ttl-and-timestamp\">documentation</a>)</li>\n<li>(pre-2.5.0) когда нам нужно выполнить эффективный join с данными в Cassandra</li>\n<li>когда мы хотим удалить данные из таблицы (<a href=\"https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/5_saving.md#deleting-rows-and-columns\">документация</a>)</li>\n<li>когда мы хотим выполнить частичное обновление колонок с типами данных <code>list</code>/<code>map</code>/<code>set</code> (добавить или удалить отдельные элементы)</li>\n</ul>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1590660402920_-1417068235",
      "id": "20200304-103507_803705969",
      "dateCreated": "2020-05-28T10:06:42+0000",
      "dateStarted": "2020-07-13T07:55:37+0000",
      "dateFinished": "2020-07-13T07:55:37+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8239"
    },
    {
      "title": "Чтение данных в RDD[CassandraRow]",
      "text": "%spark\n\nimport com.datastax.spark.connector._\n\nval data = sc.cassandraTable(\"test\", \"jtest\")",
      "user": "anonymous",
      "dateUpdated": "2020-05-29T09:49:14+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import com.datastax.spark.connector._\n\u001b[1m\u001b[34mdata\u001b[0m: \u001b[1m\u001b[32mcom.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow]\u001b[0m = CassandraTableScanRDD[1385] at RDD at CassandraRDD.scala:19\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1590682635924_883647425",
      "id": "paragraph_1590682635924_883647425",
      "dateCreated": "2020-05-28T16:17:15+0000",
      "dateStarted": "2020-05-29T09:49:14+0000",
      "dateFinished": "2020-05-29T09:49:15+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8240"
    },
    {
      "title": "Запись данных в Cassandra",
      "text": "%spark\ndata.saveToCassandra(\"test\", \"jtest\")",
      "user": "anonymous",
      "dateUpdated": "2020-05-29T09:49:47+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1590682954084_-918753717",
      "id": "paragraph_1590682954084_-918753717",
      "dateCreated": "2020-05-28T16:22:34+0000",
      "dateStarted": "2020-05-29T09:49:47+0000",
      "dateFinished": "2020-05-29T09:49:49+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8242"
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2020-05-29T08:39:04+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1590741544473_-521640328",
      "id": "paragraph_1590741544473_-521640328",
      "dateCreated": "2020-05-29T08:39:04+0000",
      "status": "READY",
      "$$hashKey": "object:8243"
    }
  ],
  "name": "Cassandra Day Russia: Spark + Cassandra",
  "id": "2FCA226PC",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-preview1",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/Cassandra Day Russia: Spark + Cassandra"
}